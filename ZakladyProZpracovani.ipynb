{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaTokenizerFast, TFRobertaModel\n",
    "from transformers.optimization_tf import create_optimizer, WarmUp\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras_bert import gelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import datetime as dt\n",
    "from pprint import pformat\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "import seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, TimeDistributed, Bidirectional, LSTM, Concatenate, Conv1D, Dropout, Dot, Lambda, GlobalMaxPool1D, GlobalAvgPool1D, Add, GaussianNoise, Embedding, RepeatVector, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from keras_pos_embd import TrigPosEmbedding, PositionEmbedding\n",
    "from keras_bert.layers import TokenEmbedding\n",
    "\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isProtein = False\n",
    "isCLS = False\n",
    "\n",
    "import string\n",
    "\n",
    "\n",
    "tokenizer_dict = {}\n",
    "tokenizer_dict[\"<pad>\"] = 0\n",
    "tokenizer_dict[\"<mask>\"] = 1\n",
    "\n",
    "if(not isProtein):\n",
    "    #speslZnaky = \"áéíóúýůžščřťďěň°®…”“„’ì»�–§•✔üß€đĐ$öëä\"\n",
    "    speslZnaky = \"áéíóúýůžščřťďěňüöëä\"\n",
    "\n",
    "    for aa in string.printable.lower()+speslZnaky:\n",
    "        try:\n",
    "            tokenizer_dict[aa]\n",
    "        except:\n",
    "            tokenizer_dict[aa] = len(tokenizer_dict)\n",
    "\n",
    "    tokenizer_dict[0] = 2\n",
    "    tokenizer_dict[1] = 3\n",
    "    tokenizer_dict[2] = 4\n",
    "    tokenizer_dict[3] = 5\n",
    "    tokenizer_dict[4] = 6\n",
    "    tokenizer_dict[5] = 7\n",
    "    tokenizer_dict[6] = 8\n",
    "    tokenizer_dict[7] = 9\n",
    "    tokenizer_dict[8] = 10\n",
    "    tokenizer_dict[9] = 11\n",
    "    tokenizer_dict[\"\\u200b\"] =70\n",
    "    tokenizer_dict[\"\\uf02d\"] =70\n",
    "    tokenizer_dict[\"ø\"]=85\n",
    "    tokenizer_dict[\"ù\"]=81\n",
    "    tokenizer_dict[\"è\"]=84\n",
    "    tokenizer_dict[\"ì\"]=88\n",
    "    tokenizer_dict[\"<unknown>\"] =  len(tokenizer_dict)\n",
    "    unknown_token_id = tokenizer_dict[\"<unknown>\"]\n",
    "else:\n",
    "    for aa in string.ascii_uppercase:\n",
    "        tokenizer_dict[aa] = len(tokenizer_dict)\n",
    "    tokenizer_dict[\" \"] = tokenizer_dict[\"<pad>\"] \n",
    "\n",
    "pad_token = \"<pad>\"\n",
    "pad_token_id = tokenizer_dict[\"<pad>\"]\n",
    "mask_token = \"<mask>\"\n",
    "mask_token_id = tokenizer_dict[\"<mask>\"]\n",
    "#unknown_token_id = tokenizer_dict[\"<unknown>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(isProtein):\n",
    "    MODEL_DIR = f\"Protein_proteopairs13-ml{MAX_LEN}-mask\" + ( \"-cls\" if (isCLS) else \"\")\n",
    "else:\n",
    "    MODEL_DIR = f\"Word_proteopairs13-ml{MAX_LEN}-mask\" + ( \"-cls\" if (isCLS) else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_DIR is not None and os.path.isdir(MODEL_DIR) :\n",
    "    print(\"Loading model from\", MODEL_DIR)\n",
    "    model = load_model(os.path.join(MODEL_DIR, \"model\"), custom_objects={\"WarmUp\": WarmUp, \"siamese_loss\":siamese_loss})\n",
    "\n",
    "\n",
    "    with open(f\"{MODEL_DIR}/config.json\", \"r\", encoding=\"utf-8\") as fr:\n",
    "        TRAIN_STATE = json.load(fr)\n",
    "        INITIAL_EPOCH = TRAIN_STATE[\"INITIAL_EPOCH\"]\n",
    "    print(f\"Clasification will use model with TrainState of: {TRAIN_STATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model.optimizer is None:\n",
    "    print(\"Compiling model with an optimizer\")\n",
    "    optimizer, lr_schedule = create_optimizer(init_lr=LR_START, \n",
    "                                 num_train_steps=TRAIN_STEPS*EPOCHS, \n",
    "                                 num_warmup_steps=TRAIN_STEPS*WARMUP,\n",
    "                                 )\n",
    "else:\n",
    "    print(\"Skipped model compilation, optimizer already initialized\")\n",
    "    optimizer = model.optimizer\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss={\"out\": siamese_loss,\n",
    "                    \"pos_out\": \"binary_crossentropy\",\n",
    "                    \"neg_out\": \"binary_crossentropy\",\n",
    "                    \"total_out\": \"binary_crossentropy\",\n",
    "                },\n",
    "               metrics={\n",
    "                    \"pos_out\": \"accuracy\",\n",
    "                    \"neg_out\": \"accuracy\",\n",
    "                    \"total_out\": \"accuracy\",\n",
    "               },\n",
    "              loss_weights={\n",
    "                    \"out\": 1.0,\n",
    "                    \"pos_out\": 0.1,\n",
    "                    \"neg_out\": 0.1,\n",
    "                    \"total_out\": 0.,\n",
    "              }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelEmbedding  = model.get_layer(name= \"embedding\")\n",
    "modelConvLayer = model.get_layer(name= \"conv1d\")\n",
    "modelEmbedder = model.get_layer(name= \"embedder\")\n",
    "modelDist = model.get_layer(name= \"dist_model\")\n",
    "print(modelDist.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vlastni_vectorize_batch(batch, mask=False):\n",
    "    tokens_pos = []\n",
    "\n",
    "\n",
    "    for pos in batch:\n",
    "        #print(pos)\n",
    "        pos_t1 = vectorize_seq(pos, MAX_LEN, mask)        \n",
    "        tokens_pos.append(pos_t1)\n",
    "        \n",
    "        \n",
    "    pos1, pos2 = modelEmbedder(modelConvLayer(modelEmbedding(pad_sequences(tokens_pos, maxlen=MAX_LEN, dtype=np.int32, padding=\"post\", value=pad_token_id))))\n",
    "   \n",
    "    \n",
    "    #targets = np.array(targets)\n",
    "    #pos_out = np.ones_like(targets)\n",
    "    #neg_out = np.zeros_like(targets)\n",
    "\n",
    "    return (#{\"pos1\": pos1,\"pos2\": pos2},\n",
    "            [pos1,pos2]\n",
    "           # {\"out\": targets, \"pos_out\": pos_out, \"neg_out\": neg_out, \"total_out\": pos_out},\n",
    "\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mapa(vstup,exportName:str = \"\",title:str = \"\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    viridis = mpl.colormaps['viridis'].resampled(256)\n",
    "    newcolors = viridis(np.linspace(0, 1, 256))\n",
    "    pink = np.array([248/256, 24/256, 148/256, 1])\n",
    "    black = np.array([0/256, 0/256, 0/256, 1])\n",
    "    newcolors[-16:, :] = pink\n",
    "    newcolors[:16,:] = black\n",
    "\n",
    "\n",
    "    ax = seaborn.heatmap(vstup,annot =False, cmap =  mpl.colors.ListedColormap(newcolors))\n",
    "    ax.set(xlabel=\"\", ylabel=\"\",title=title)\n",
    "    ax.xaxis.tick_top()\n",
    "    plt.xticks(rotation=90)\n",
    "    if(exportName != \"\"):\n",
    "        plt.savefig(exportName,bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    #TODO: SaveFig\n",
    "    #plt.savefig(\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "class Embeddings_ready():\n",
    "    \n",
    "    arr_embeddings = []\n",
    "    strings = []\n",
    "    \n",
    "    def __init__(self, path:str = \"\",embeddings= [],text:str =\"\"):\n",
    "        if(path != \"\"):\n",
    "            self.loadPickle(path)\n",
    "        elif(text != \"\"):\n",
    "            self.arr_embeddings = self.prepEmbeddings(text)\n",
    "\n",
    "        elif(len(embeddings)>0):\n",
    "            self.arr_embeddings = embeddings\n",
    "            \n",
    "        \n",
    "    def loadPickle(self,path:str):\n",
    "        with open(path,\"rb\") as p:\n",
    "            self.arr_embeddings= pickle.load(p)\n",
    "        \n",
    "    def savePickle(self, path:str):\n",
    "        with open(path,\"wb\") as p:\n",
    "            return pickle.dump(self.arr_embeddings,p)\n",
    "        \n",
    "    def getEmbeddings(self):\n",
    "        return self.arr_embeddings\n",
    "    \n",
    "    #firstSEC, secondSEC - označuje zda používáme z embeddings \"pos1\" nebo \"pos2\" sloupec - 0 = pos1, 1 = pos2\n",
    "    def getMapReadyEmbeddings(self, anotherClass = None,firstSEC:int = 1, secondSEC:int = 0):\n",
    "        retVal = []\n",
    "        if(anotherClass == None):\n",
    "            anotherClass = self\n",
    "        \n",
    "       \n",
    "        for i in range(len(self.arr_embeddings[0])):\n",
    "            retVal.append([])\n",
    "            #print(len(retVal))\n",
    "            for j in range(len(anotherClass.arr_embeddings[0])):\n",
    "                retVal[i].append([self.arr_embeddings[firstSEC][i:i+1],anotherClass.arr_embeddings[secondSEC][j:j+1]])\n",
    "                #retVal[i].\n",
    "                next\n",
    "        return retVal\n",
    "                \n",
    "    def prepEmbeddings(self,text:str,n_Chars:int = 1):\n",
    "        \n",
    "        splitted = textSplitter(text,str(n_Chars))\n",
    "        self.strings = splitted\n",
    "\n",
    "        embedded = vlastni_vectorize_batch(splitted)\n",
    "        return embedded\n",
    "    \n",
    "    \n",
    "    def generateMap(self,matrix=None,firstSEC:int = 1, secondSEC:int = 0):\n",
    "        if(matrix == None):\n",
    "            matrix = self.getMapReadyEmbeddings(firstSEC=firstSEC, secondSEC=secondSEC)\n",
    "        retVal = []\n",
    "        for i in range(np.size(matrix,0)):\n",
    "            retVal.append([])\n",
    "            for j in range(np.size(matrix,1)):\n",
    "                retVal[i].append(modelDist(matrix[i][j])[1][0][0].numpy()[0])\n",
    "        #print(modelDist(matrix[i][j])[1][0][0].numpy()[0])\n",
    "        return retVal\n",
    "        \n",
    "        \n",
    "        #Asi zatím nemá smysl, aby existoval:\n",
    "    def getWords(self,number:int = 0):\n",
    "        if(number == 0):\n",
    "            if(self.words1==None):\n",
    "                return None #Asi by měl Rasinout exception?\n",
    "            return self.words1\n",
    "\n",
    "\n",
    "        elif(number == 1):\n",
    "            if(self.words2 == None):\n",
    "                if(self.words1 == None):\n",
    "                    return None #Asi by měl Rasinout exception?\n",
    "                return self.words1\n",
    "            return self.words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import blake2b \n",
    "class MapResult():\n",
    "    \n",
    "    import pickle\n",
    "    import math\n",
    "    statistika = None\n",
    "    words1= None\n",
    "    words2 = None\n",
    "    emb1 = \"\"\n",
    "    emb2 = \"\"\n",
    "    vzdalenostniMapa = None\n",
    "    embeddingy = None\n",
    "    dtype = [(\"avg\",float),(\"index\",float),(\"nRecords\",float)]\n",
    "    words = None \n",
    "    \n",
    "    def __init__(self,emb1 = \"\", emb2 = \"\", words1 = None, words2=None,vzdalenostniMapa=None,text = \"\"):\n",
    "        self.emb1 = emb1\n",
    "        self.emb2 = emb2\n",
    "        self.words1 = words1\n",
    "        self.words2 = words2\n",
    "        self.vzdalenostniMapa = vzdalenostniMapa\n",
    "        if(text != \"\"):\n",
    "            self.words = text\n",
    "            embeddingy = Embeddings_ready(text=text)\n",
    "            #self.emb1 = embeddingy\n",
    "            #self.emb1 = #TODO - Hledání + načítání z disku podle názvu / hashe / db?... prostě něco\n",
    "            self.vzdalenostniMapa = embeddingy.generateMap()\n",
    "    \n",
    "    def toString(self):\n",
    "        return(\"words1: \" + str(self.words1 or \"None\") + \"\\n\" +\n",
    "              \"words2: \" + str(self.words2 or \"None\") + \"\\n\" +\n",
    "               \"emb1: \" + str(self.emb1) + \"\\n\"+\n",
    "               \"emb2: \" + str(self.emb2) + \"\\n\"+\n",
    "               \"statistika: \" + str(self.statistika or \"None\") + \"\\n\"+\n",
    "               \"vzdalenostniMapa: \" + str(self.vzdalenostniMapa or \"None\") + \"\\n\"              )\n",
    "        \n",
    "  \n",
    "    \n",
    " \n",
    "    def getHash(self, words:str = None):\n",
    "        hasher = blake2b()\n",
    "        if (self != None):\n",
    "            if(self.words == None):\n",
    "                words = self.words1+self.words2\n",
    "            else: \n",
    "                words=self.words\n",
    "            \n",
    "        hasher.update(bytes(words,\"UTF-8\"))\n",
    "        return hasher.hexdigest()\n",
    "    \n",
    "    def getEmbeddings(self,number:int = 0):\n",
    "        if(number == 0):\n",
    "            if(self.emb1==\"\"):\n",
    "                return None #Asi by měl Raisnout Exception\n",
    "            return pickle.load(open(self.emb1,\"rb\"))\n",
    "                \n",
    "        elif(number == 1):\n",
    "            if(self.emb2==\"\"):\n",
    "                if(self.emb1==\"\"):\n",
    "                    return None #Asi by měl Raisnout Exception\n",
    "                return pickle.load(open(self.emb1,\"rb\"))\n",
    "            return pickle.load(open(self.emb2,\"rb\")) \n",
    "            \n",
    "        return None #DEFAULT \n",
    "    \n",
    "    def getVzdalenostniMapa(self):\n",
    "        if(self.vzdalenostniMapa == None):\n",
    "            self.generateMap()\n",
    "        return self.vzdalenostniMapa\n",
    "    \n",
    "    def generateMap(self,matrix=None,firstSEC:int = 0, secondSEC:int = 1):\n",
    "        if(matrix == None):\n",
    "            \n",
    "            matrix = self.getEmbeddings().getMapReadyEmbeddings(anotherClass=self.getEmbeddings(number=1),\n",
    "                                                                firstSEC=firstSEC,\n",
    "                                                                secondSEC=secondSEC)\n",
    "        retVal = []\n",
    "        for i in range(np.size(matrix,0)):\n",
    "            retVal.append([])\n",
    "            for j in range(np.size(matrix,1)):\n",
    "                retVal[i].append(modelDist(matrix[i][j])[1][0][0].numpy()[0])\n",
    "        #print(modelDist(matrix[i][j])[1][0][0].numpy()[0])\n",
    "        self.vzdalenostniMapa= retVal\n",
    "    \n",
    "    \n",
    "    def Statistics(self, mapaData=None):\n",
    "        #TODO: Další statistické funkce, možná úprava ztrátové funkce, či něco dalšího\n",
    "        if(mapaData ==None):\n",
    "            mapaData = self.getVzdalenostniMapa()\n",
    "        nRecords = np.size(mapaData,0)-1\n",
    "        rangeMax = round(nRecords/2)\n",
    "        #best_avg = 0\n",
    "        #pos_avg = 0\n",
    "        mapaData2 = np.array(mapaData)\n",
    "        all_Avgs = []\n",
    "        for i in range(-rangeMax,rangeMax): \n",
    "            #Validation_bias - \"ztrátová funkce\" pro průměry mimo očekávanou osu\n",
    "            #validation_bias = 1-abs((i+30)/nRecords)\n",
    "            all_Avgs.append((np.average(mapaData2.diagonal(i)),i,rangeMax-i))\n",
    "        return all_Avgs\n",
    "    \n",
    "    def prumery(self,statistika):\n",
    "        np.flip(np.sort(np.array(statistika,dtype=self.dtype),order=\"avg\"))\n",
    "    \n",
    "    def pickleThis(self,path:str = \"./Pickles\"):\n",
    "        hash = self.getHash()\n",
    "        with open(path+\"/\"+hash+\".pickle\",\"wb\") as p:\n",
    "            pickle.dump(self,p)\n",
    "            p.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GenerateMap(vstup1, vstup2 = \"\", delic=\"1\"):\n",
    "    if(vstup2 == \"\"):\n",
    "        vstup2 =vstup1\n",
    "    splittedString1 = textSplitter(vstup1,delic)\n",
    "    splittedString2 = textSplitter(vstup2,delic)\n",
    "    batch = []\n",
    "\n",
    "    delka1 = len(splittedString1)                \n",
    "    delka2 = len(splittedString2)\n",
    "\n",
    "    #print(splittedString1)\n",
    "\n",
    "\n",
    "    mapa = np.zeros((delka1,delka2))\n",
    "    mapa_vstup1 = np.zeros((delka1,delka1))\n",
    "    mapa_vstup2 = np.zeros((delka2,delka2))\n",
    "\n",
    "    for i in range(delka1-1):\n",
    "        #X\n",
    "        for j in range(delka2-1):\n",
    "            #Y\n",
    "            #print(splittedString[i] + \" \" + splittedString[i+1]     )\n",
    "            batch = []\n",
    "            batch.append(((splittedString1[i],splittedString1[i+1],splittedString2[j],splittedString2[j+1]),0))\n",
    "            vectorized = vectorize_batch(batch,mask=False)        \n",
    "            out = model.predict(vectorized)\n",
    "            #mapa[i,j+1] = out[\"out\"][0][3]\n",
    "            #mapa[i+1,j] = out[\"out\"][0][2]\n",
    "            #mapa_vstup1[i,i+1] = out[\"out\"][0][0] \n",
    "            #mapa_vstup2[j,j+1] = out[\"out\"][0][1] \n",
    "\n",
    "            mapa[i,j+1] = out[\"neg_out\"][0][0][0]\n",
    "           # mapa[i+1,j] = out[\"out\"][0][2]\n",
    "            mapa_vstup1[i,i+1] = out[\"pos_out\"][0][0][0]\n",
    "          #  mapa_vstup2[j,j+1] = out[\"out\"][0][1] \n",
    "\n",
    "    mapa[0,0] = model.predict(vectorized,batch_size = 32)[\"out\"][0][0]\n",
    "\n",
    "\n",
    "    mapa_vstup1 = heatmapa(vstup1,vstup1,delic)\n",
    "    mapa_vstup2 = heatmapa(vstup2,vstup2,delic)\n",
    "\n",
    "\n",
    "    #for i in out[\"out\"]:\n",
    "     #   mapa[i,:] = out[\"out\"][i*delka2:i*delka2+delka2-1]\n",
    "    #\n",
    "    #    d_pos1, pos_out = dist_model([pos1b, pos2a])\n",
    "     #   d_pos2, pos_out2 = dist_model([neg1b, neg2a])\n",
    "\n",
    "      #  d_neg1, neg_out = dist_model([pos1b, neg2a])\n",
    "       # d_neg2, neg_out2 = dist_model([neg1b, pos2a])\n",
    "\n",
    "\n",
    "\n",
    "    zmenenaMapa = pd.DataFrame(data = mapa, index = splittedString1, columns = splittedString2)\n",
    "    zmenenaMapa_vstup1 = pd.DataFrame(data = mapa_vstup1, index = splittedString1, columns = splittedString1)\n",
    "    zmenenaMapa_vstup2 = pd.DataFrame(data = mapa_vstup2, index = splittedString2, columns = splittedString2)\n",
    "    Mapa(zmenenaMapa)\n",
    "    Mapa(zmenenaMapa_vstup1)\n",
    "    Mapa(zmenenaMapa_vstup2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import os\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "done = 0\n",
    "skippedDone = 0\n",
    "tooLong = 0\n",
    "count = 0\n",
    "\n",
    "inputText = \"\"\n",
    "pickleRoute = \"./Pickles/\"\n",
    "if(isProtein):\n",
    "    inputText = \"uniref90_tax-free_shuffled_val.tsv\"\n",
    "    pickleRoute +=\"Protein\"\n",
    "else:\n",
    "    inputText = \"CC-aug2018-oct2021_ces.10G_val.tsv\"\n",
    "    pickleRoute +=\"Words\"\n",
    "    \n",
    "pickleRoute  += ( \"CLS\" if (isCLS) else \"\")\n",
    "\n",
    "with open(\"./\"+inputText,\"r\",encoding = \"utf8\") as reader:\n",
    " #testovaci_vzor.txt\n",
    "#with open(\"./testovaci_vzor.txt\",\"r\",encoding = \"utf8\") as reader:\n",
    "    startTime = time.time()\n",
    "    #sumTime = 0\n",
    "    for i in reader:\n",
    "        if(count == 0):\n",
    "            count +=1\n",
    "            continue\n",
    "        \n",
    "        vstup = i.split('\\t')[2]\n",
    "        if(len(vstup)< 1200):\n",
    "            \n",
    "\n",
    "            print(\"Current Len: \" , str(len(vstup)), \"(+32) / Avg: \", str((time.time()-startTime)/done) if (not done == 0) else 0, \" secs\" )\n",
    "            #print(vstup)\n",
    "            if(exists(pickleRoute+\"/\"+MapResult.getHash(self = None, words = vstup)+\".pickle\")):\n",
    "                #print(\"-\",end=\"\")\n",
    "                clear_output(wait=True)\n",
    "                skippedDone += 1\n",
    "                print(\"Done:\",str(done+skippedDone), \" / \" ,str(done), \" / \" ,str(skippedDone) )\n",
    "                print(\"Too Long Skipped:\" ,str(tooLong))\n",
    "                \n",
    "                continue\n",
    "\n",
    "\n",
    "            vysl = MapResult(text=vstup)\n",
    "            vysl.pickleThis(path = pickleRoute)\n",
    "            #print(\".\", end =\"\")\n",
    "            done += 1\n",
    "        else:\n",
    "            #print(\"*\", end =\"\")\n",
    "            tooLong += 1\n",
    "        \n",
    "        count +=1\n",
    "        #os.system('cls')\n",
    "        clear_output(wait=True)\n",
    "        print(\"Done:\",str(done+skippedDone), \" / \" ,str(done), \" / \" ,str(skippedDone) )\n",
    "        print(\"Too Long Skipped:\" ,str(tooLong))\n",
    "        \n",
    "    #print(reader.readline().split('\\t')[2])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
